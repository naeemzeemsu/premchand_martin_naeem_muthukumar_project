{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5if8juGWYn4"
      },
      "outputs": [],
      "source": [
        "# CSE 404 Machine Learning Project NFL combine data to HOF percentage\n",
        "# Group Members: Pranav Premchand, Daphne Martin, Zeeshan Naeem, Pranesh Muthukumar\n",
        "\n",
        "# Data in combine data folder named as qb combine data.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVR\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Step 1: Load and prepare the data\n",
        "data = pd.read_csv(\"qb_combine_data.csv\")  # Load the QB combine data\n",
        "\n",
        "# Preprocess height column to separate feet and inches\n",
        "data['Height_Feet'] = data['Ht'].apply(lambda x: int(x.split(\"'\")[0]))\n",
        "data['Height_Inches'] = data['Ht'].apply(lambda x: int(x.split(\"'\")[1].replace('\"', '')))\n",
        "\n",
        "X = data[['Height_Feet', 'Height_Inches', 'Wt', '40yd', 'Vertical', 'Broad Jump', '3Cone', 'Shuttle']]  # Features\n",
        "y = data['Hof']  # Target variable (continuous)\n",
        "\n",
        "# Normalize the features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2.1: Handle missing values with mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "# Step 3: Build and train the support vector regression model\n",
        "svr_model = SVR()  # Default SVR\n",
        "svr_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 4: Build and train the Ridge (L2 regularization) model\n",
        "ridge_model = Ridge(alpha=0.1)  # You can adjust the alpha parameter for tuning the strength of regularization\n",
        "ridge_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 5: Build and train the Lasso (L1 regularization) model\n",
        "lasso_model = Lasso(alpha=0.1)  # You can adjust the alpha parameter for tuning the strength of regularization\n",
        "lasso_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 6: Build the neural network regression model with increased complexity and more advanced architecture\n",
        "nn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_imputed.shape[1],)),  # Input layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(64, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(32, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(16, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Step 7: Compile the neural network model\n",
        "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Step 8: Train the neural network model\n",
        "nn_model.fit(X_train_imputed, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Step 9: Evaluate the models\n",
        "svr_mse = mean_squared_error(y_test, svr_model.predict(X_test_imputed))\n",
        "ridge_mse = mean_squared_error(y_test, ridge_model.predict(X_test_imputed))\n",
        "lasso_mse = mean_squared_error(y_test, lasso_model.predict(X_test_imputed))\n",
        "nn_mse = mean_squared_error(y_test, nn_model.predict(X_test_imputed))\n",
        "\n",
        "print(\"Support Vector Regression Mean Squared Error:\", svr_mse)\n",
        "print(\"Ridge Regression Mean Squared Error:\", ridge_mse)\n",
        "print(\"Lasso Regression Mean Squared Error:\", lasso_mse)\n",
        "print(\"Neural Network Mean Squared Error:\", nn_mse)\n",
        "\n",
        "# Step 10: Define function to predict HOF percentage for a single player's combine data\n",
        "def predict_hof_percentage(height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle):\n",
        "    new_data = pd.DataFrame([[height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle]],\n",
        "                            columns=['Height_Feet', 'Height_Inches', 'Wt', '40yd', 'Vertical', 'Broad Jump', '3Cone', 'Shuttle'])\n",
        "    new_data_imputed = imputer.transform(new_data)\n",
        "    return svr_model.predict(new_data_imputed)[0]\n",
        "\n",
        "# Step 11: Example usage of the prediction function\n",
        "height_feet = 6\n",
        "height_inches = 4\n",
        "weight = 211  # pounds\n",
        "forty_yard = 5.28  # seconds\n",
        "vertical_jump = 24.5  # inches\n",
        "broad_jump = 99  # inches\n",
        "three_cone = 7.2  # seconds\n",
        "shuttle = 4.38  # seconds\n",
        "\n",
        "hof_percentage = predict_hof_percentage(height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle)\n",
        "print(\"Predicted Hall of Fame Percentage:\", (hof_percentage)*100)\n"
      ],
      "metadata": {
        "id": "vuS8zw5ldnKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefa548b-9ee2-49b6-f722-56aa1322ce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 4s 287ms/step - loss: 0.0242 - val_loss: 0.0723\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0256 - val_loss: 0.0707\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0192 - val_loss: 0.0700\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0208 - val_loss: 0.0699\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0196 - val_loss: 0.0701\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0177 - val_loss: 0.0702\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0168 - val_loss: 0.0704\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0186 - val_loss: 0.0701\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0174 - val_loss: 0.0698\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0175 - val_loss: 0.0695\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0171 - val_loss: 0.0692\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0189 - val_loss: 0.0690\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0146 - val_loss: 0.0689\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0166 - val_loss: 0.0687\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0173 - val_loss: 0.0689\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0161 - val_loss: 0.0691\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0181 - val_loss: 0.0692\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0167 - val_loss: 0.0691\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0160 - val_loss: 0.0691\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0163 - val_loss: 0.0691\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Support Vector Regression Mean Squared Error: 0.06722190943708854\n",
            "Ridge Regression Mean Squared Error: 0.06692971488383508\n",
            "Lasso Regression Mean Squared Error: 0.0672040207656592\n",
            "Neural Network Mean Squared Error: 0.06997020096292247\n",
            "Predicted Hall of Fame Percentage: 12.913799824688615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7d42yuU2DjHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}