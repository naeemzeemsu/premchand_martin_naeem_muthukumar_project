{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5if8juGWYn4"
      },
      "outputs": [],
      "source": [
        "# CSE 404 Machine Learning Project NFL combine data to HOF percentage\n",
        "# Group Members: Pranav Premchand, Daphne Martin, Zeeshan Naeem, Pranesh Muthukumar\n",
        "\n",
        "# Data in combine data folder named as qb combine data.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "\n",
        "# Step 1: Load and prepare the data\n",
        "data = pd.read_csv(\"qb_combine_data.csv\")  # Load the QB combine data\n",
        "\n",
        "# Preprocess height column to separate feet and inches\n",
        "data['Height_Feet'] = data['Ht'].apply(lambda x: int(x.split(\"'\")[0]))\n",
        "data['Height_Inches'] = data['Ht'].apply(lambda x: int(x.split(\"'\")[1].replace('\"', '')))\n",
        "\n",
        "X = data[['Height_Feet', 'Height_Inches', 'Wt', '40yd', 'Vertical', 'Broad Jump', '3Cone', 'Shuttle']]  # Features\n",
        "y = data['Hof']  # Target variable (continuous)\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2.1: Handle missing values with mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Step 3: Build and train the support vector regression model\n",
        "svr_model = SVR()  # Default SVR\n",
        "svr_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 4: Build and train the Ridge (L2 regularization) model\n",
        "ridge_model = Ridge(alpha=0.1)  # You can adjust the alpha parameter for tuning the strength of regularization\n",
        "ridge_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 5: Build and train the Lasso (L1 regularization) model\n",
        "lasso_model = Lasso(alpha=0.1)  # You can adjust the alpha parameter for tuning the strength of regularization\n",
        "lasso_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Step 6: Build and train the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),  # LSTM layer\n",
        "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "lstm_model.fit(X_train_reshaped, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Step 7: Evaluate the models\n",
        "svr_mse = mean_squared_error(y_test, svr_model.predict(X_test_imputed))\n",
        "ridge_mse = mean_squared_error(y_test, ridge_model.predict(X_test_imputed))\n",
        "lasso_mse = mean_squared_error(y_test, lasso_model.predict(X_test_imputed))\n",
        "lstm_mse = mean_squared_error(y_test, lstm_model.predict(X_test_reshaped))\n",
        "\n",
        "print(\"Support Vector Regression Mean Squared Error:\", svr_mse)\n",
        "print(\"Ridge Regression Mean Squared Error:\", ridge_mse)\n",
        "print(\"Lasso Regression Mean Squared Error:\", lasso_mse)\n",
        "print(\"LSTM Mean Squared Error:\", lstm_mse)\n",
        "\n",
        "# Step 8: Define function to predict HOF percentage for a single player's combine data\n",
        "def predict_hof_percentage(height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle):\n",
        "    new_data = pd.DataFrame([[height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle]],\n",
        "                            columns=['Height_Feet', 'Height_Inches', 'Wt', '40yd', 'Vertical', 'Broad Jump', '3Cone', 'Shuttle'])\n",
        "    new_data_imputed = imputer.transform(new_data)\n",
        "    new_data_scaled = scaler.transform(new_data_imputed)\n",
        "    new_data_reshaped = new_data_scaled.reshape(1, 1, new_data_scaled.shape[1])\n",
        "    return svr_model.predict(new_data_imputed)[0], ridge_model.predict(new_data_imputed)[0], lasso_model.predict(new_data_imputed)[0], lstm_model.predict(new_data_reshaped)[0]\n",
        "\n",
        "# Step 9: Example usage of the prediction function\n",
        "height_feet = 6\n",
        "height_inches = 4\n",
        "weight = 211  # pounds\n",
        "forty_yard = 5.28  # seconds\n",
        "vertical_jump = 24.5  # inches\n",
        "broad_jump = 99  # inches\n",
        "three_cone = 7.2  # seconds\n",
        "shuttle = 4.38  # seconds\n",
        "\n",
        "svr_hof_percentage, ridge_hof_percentage, lasso_hof_percentage, lstm_hof_percentage = predict_hof_percentage(height_feet, height_inches, weight, forty_yard, vertical_jump, broad_jump, three_cone, shuttle)\n",
        "print(\"Predicted Hall of Fame Percentage (SVR):\", svr_hof_percentage * 100)\n",
        "print(\"Predicted Hall of Fame Percentage (Ridge):\", ridge_hof_percentage * 100)\n",
        "print(\"Predicted Hall of Fame Percentage (Lasso):\", lasso_hof_percentage * 100)\n",
        "print(\"Predicted Hall of Fame Percentage (LSTM):\", lstm_hof_percentage * 100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vuS8zw5ldnKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c86f960-e7a0-4642-f105-39cc553714fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 2s 250ms/step - loss: 0.0287 - val_loss: 0.0792\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.0255 - val_loss: 0.0748\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0213 - val_loss: 0.0719\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.0207 - val_loss: 0.0696\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0187 - val_loss: 0.0683\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0182 - val_loss: 0.0678\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0177 - val_loss: 0.0677\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.0168 - val_loss: 0.0679\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0176 - val_loss: 0.0682\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.0170 - val_loss: 0.0684\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0174 - val_loss: 0.0687\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0185 - val_loss: 0.0690\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0177 - val_loss: 0.0692\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0182 - val_loss: 0.0695\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.0172 - val_loss: 0.0696\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0171 - val_loss: 0.0696\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0172 - val_loss: 0.0697\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.0157 - val_loss: 0.0695\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.0167 - val_loss: 0.0694\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.0175 - val_loss: 0.0691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e7e88a4a8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 8ms/step\n",
            "Support Vector Regression Mean Squared Error: 0.06390573231342837\n",
            "Ridge Regression Mean Squared Error: 0.0669366343869932\n",
            "Lasso Regression Mean Squared Error: 0.0672040207656592\n",
            "LSTM Mean Squared Error: 0.07118060810619492\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted Hall of Fame Percentage (SVR): 10.019855615576006\n",
            "Predicted Hall of Fame Percentage (Ridge): 5.565339933277684\n",
            "Predicted Hall of Fame Percentage (Lasso): 5.073319327731092\n",
            "Predicted Hall of Fame Percentage (LSTM): [15.684931]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7d42yuU2DjHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}